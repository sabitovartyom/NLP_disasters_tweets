# NLP Disasters Tweets

Этот репозиторий содержит решение [задачи классификации твитов на тему катастроф](https://www.kaggle.com/competitions/nlp-getting-started/data) с использованием моделей NLP (в частности, BERT для создания эмбеддингов и нейронной сети для классификации). Проект включает подготовку данных, создание эмбеддингов и обучение модели для предсказания меток. Метрика f1 полученного результата 0,80753 (365/1008 место в соревновании). 

## Описание задачи

Цель проекта — разработка модели машинного обучения для бинарной классификации твитов, определяющей, связан ли твит с реальной катастрофой (`target=1`) или нет (`target=0`). Используется датасет, содержащий твиты (`train.csv` и `test.csv`), а также предобученная модель BERT для создания эмбеддингов.

## Содержимое репозитория

| Файл                               | Описание                                                                 |
|------------------------------------|--------------------------------------------------------------------------|
| `test.csv`                         | Тестовый датасет с твитами (без меток).                                  |
| `train.csv`                        | Обучающий датасет с твитами и метками (`target`).                        |
| `preprocess.ipynb`                 | Jupyter Notebook с кодом предобработки данных и создания эмбеддингов.    |
| `process.ipynb`                    | Jupyter Notebook с кодом обучения модели и создания предсказаний.        |
| `submission.csv`                   | Файл с предсказаниями для тестового набора данных (лучшее решение).      |
| `test_embeddings.csv`              | Эмбеддинги BERT для тестового набора.                                    |
| `train_embeddings_and_targets.csv` | Эмбеддинги BERT для обучающего набора с метками.                         |


## Использование

1. **Предобработка данных**:
   - Запустите `preprocess.ipynb`, чтобы:
     - Загрузить `train.csv` и `test.csv`.
     - Создать эмбеддинги BERT и сохранить их в `train_embeddings_and_targets.csv` и `test_embeddings.csv`.

2. **Обучение модели и предсказания**:
   - Запустите `process.ipynb`, чтобы:
     - Обучить нейронную сеть (`SimpleMLP`) на данных из `train_embeddings_and_targets.csv`.
     - Сделать предсказания для `test_embeddings.csv` и сохранить их в `submission.csv`.

3. **Формат submission**:
   - Файл `submission.csv` содержит столбцы `id` и `target`, где `target` — предсказанные метки (0 или 1).

## Входные и промежуточные данные

- **`train.csv`**:
  - Столбцы: `id`, `keyword`, `location`, `text`, `target`.
  - `target`: 0 (не катастрофа) или 1 (катастрофа).

- **`test.csv`**:
  - Столбцы: `id`, `keyword`, `location`, `text`.

- **`train_embeddings_and_targets.csv`**:
  - Содержит 768-мерные эмбеддинги BERT для каждого твита из `train.csv` (столбцы `embed_0`–`embed_767`) и столбец `target`.

- **`test_embeddings.csv`**:
  - Содержит 768-мерные эмбеддинги BERT для каждого твита из `test.csv` (столбцы `embed_0`–`embed_767`).

- **`submission.csv`**:
  - Столбцы: `id`, `target`.

## Примечания

В связи с ограниченными вычислительными ресурсами, использовались облегчённая версия BERT и элементарная нейронная сеть. При необходимости возможна замена на более современные и мощные аналоги без существенных энергозатрат.

## Автор

- **Артём Сабитов** (GitHub: [sabitovartyom](https://github.com/sabitovartyom))


